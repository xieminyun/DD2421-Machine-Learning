#### Assignment 1

**Complete the function mlParams(X， labels) that computes the ML estimates of µk and Σk for the different classes in the dataset. Use the provided functions to plot the 95%-confidence interval of the Gaussian distribution**.

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\1677670993477.png" alt="1677670993477" style="zoom: 67%;" />

#### Assignment 2

**Complete the functions computePrior(labels) and classifyBayes(X,prior,mu,sigma).**

#### Assignment 3

**Use the provided function testClassifier to test the accuracy for the vowels and iris datasets. Use plotBoundary to plot the decision boundary of the 2D iris dataset. Answer the following questions.**

#### When can a feature independence assumption be reasonable and when not?
**Resonable**
- In some types of data, such as sensor data or image data, the features may be naturally independent of each other. For example, in an image, the intensity of one pixel is often not related to the intensity of another pixel.
- When the features are generated by different sensors or measurements that are physically or spatially separated, it may be reasonable to assume that they are independent of each other.
- When the features are selected or engineered in such a way that they do not exhibit any correlations with each other, such as by using feature selection or feature extraction techniques.  

**Unreasonable**
- In many real-world datasets, the features are often correlated with each other, and ignoring these correlations can lead to inaccurate models. For example, in a dataset of housing prices, the number of bedrooms may be strongly correlated with the size of the house, which in turn may be strongly correlated with the price of the house.
- When the features interact with each other in non-linear ways in a large system, such as in many physical and biological systems, it may be unreasonable to assume that they are independent of each other. In these cases, more complex models that capture the interactions between features may be necessary.
- When the features are generated by complex processes that are not well-understood, it may be difficult to determine whether they are independent of each other or not. In these cases, it may be necessary to perform exploratory data analysis and/or use more flexible models that can capture complex dependencies between features.

#### How does the decision boundary look for the Iris dataset? How could one improve the classification results for this scenario by changing classifier or, alternatively, manipulating the data?

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\1677672565428.png" alt="1677672565428" style="zoom: 67%;" />

The decision boudry between class 0 and class 1 is perfect since they are scattered and easily classified. However, the decision boundary between class 1 and class 2 is not so good as some of the points are mixed or even overlapped. Maybe we can use the SVM because SVM could project the low-dimension data into high-dimension space by using the kernel function, which makes them easily seperated in the high-dimension space just like class 0 and class 1. Using a high variance mutliclass algorithm like random forest or boosting on the Naive Bayes classifiers are also optional.

#### Assignment 4

**Consider the W parameter and rewrite the mlParams function.**

#### Assignment 5 & 6

**Consider the W parameter and rewrite the Compute prior function. Complete the function trainBoost(base classifier, X, labels, T) and  classifyBoost(X, classifiers, alphas, Nclasses). Compute the classification accuracy of the boosted classifier on the vowels and iris data sets with Bayes version and Decision tree version. Answer the following questions:**

#### Is there any improvement in classification accuracy? Why/why not?

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\image-20230301132701261.png" alt="image-20230301132701261" style="zoom:67%;" />

Naive Bayes:  

Yes, the accuracy of the classification in both Iris and Vowel improves. This is because boosting helps decrease the bias of the Naive Bayes model.  

Decision Tree:  

The same case.

#### Plot the decision boundary of the boosted classifier on iris and compare it with that of the basic. What differences do you notice? Is the boundary of the boosted version more complex?

Naive Bayes: 

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\image-20230301131014502.png" alt="image-20230301131014502" style="zoom: 67%;" />

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\image-20230301131056288.png" alt="image-20230301131056288" style="zoom: 67%;" />

The decision boundary of the boosted classifier looks like a composite of small boundaries and with more edges.

Yes it's more complex.  

Decision Tree:

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\1677672680468.png" alt="1677672680468" style="zoom: 67%;" />

<img src="D:\DD2421-Machine-Learning\Bayes+boosting\assets\image-20230301131153097.png" alt="image-20230301131153097" style="zoom: 67%;" />

It's the same as Naive Bayes.

#### Can we make up for not using a more advanced model in the basic classifier(e.g. independent features) by using boosting?

Boosting helps decrease the bias of high bias model thus improving accuracy. But it may be useless or even have side-effect in the low bias model. And boosting can't compensate for the lack of expressiveness in the basic classifier's feature representation. If the basic classifier is limited by its feature representation, boosting may only lead to marginal improvements in performance, and we'd better use the advanced model.  

#### Assignment 7
 **If you had to pick a classifier, naive Bayes or a decision tree or the boosted versions of these, which one would you pick? Motivate from the following criteria:**
 - **Outliers**
 - **Irrelevant inputs: part of the feature space is irrelevant**
 - **Predictive power**
 - **Mixed types of data: binary, categorical or continuous features, etc.**
 -  **Scalability: the dimension of the data, D, is large or the number of instances, N, is large, or both.**  

- Outliers：Boosted Decision trees. Decision trees can handle outliers well since they partition the feature space into regions and are not affected by outliers that fall outside these regions. Naive Bayes may be influenced by outliers since it assumes independence between features. Boosted versions of decision tree can improve their robustness to outliers by combining multiple weak classifiers into a strong ensemble.

- Irrelevant inputs: part of the feature space is irrelevant: Boosted Decision Tree.  Decision trees are known for their feature selection capabilities, and they can identify and ignore irrelevant inputs. Naive Bayes can be sensitive to irrelevant features since it assumes that all features are independent. Boosted versions of decision tree can improve their ability to handle irrelevant inputs by selecting the most informative features and downweighting the less informative ones.

- Predictive power: Boosted versions of decision trees have shown to achieve high predictive power when the data is complex and are often used in real-world applications. Naive Bayes may struggle with complex dependencies between the features.  

- Mixed types of data: Both naive Bayes and decision tree classifiers are capable of handling mixed types of data. Boosted versions of these classifiers can further improve their ability to handle mixed types of data.

- Scalability: Naive Bayes is known for its scalability, and it can handle large datasets with a high number of features efficiently. Decision trees' performance can degrade when trees are too large.Boosted versions of these classifiers may require more computational resources and may not scale well